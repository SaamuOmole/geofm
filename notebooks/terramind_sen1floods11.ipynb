{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tarfile\n",
    "\n",
    "from terratorch.registry import BACKBONE_REGISTRY, TERRATORCH_BACKBONE_REGISTRY, TERRATORCH_DECODER_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Download Sen1Floods11 dataset\n",
    "Check [original publication](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf) and [here](https://github.com/cloudtostreet/Sen1Floods11) for more about the Sen1Floods11 dataset.\n",
    "\n",
    "- Make sure to uncomment the cell below to download the dataset and specify `dataset_root` for where the downloaded dataset should go.\n",
    "\n",
    "- Comment cell again after downloading to avoid unnecessarily running the download process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_root = \"/Users/samuel.omole/Desktop/repos/geofm_datasets\" # change dataset root to desired location\n",
    "# url = \"https://drive.google.com/uc?id=1lRw3X7oFNq_WyzBO6uyUJijyTuYm23VS\"\n",
    "# archive = dataset_root + \"/sen1floods11_v1.1.tar.gz\"\n",
    "# extract_dir = dataset_root + \"/sen1floods11_v1.1\"\n",
    "\n",
    "# # download if missing\n",
    "# if not os.path.isfile(archive):\n",
    "#     gdown.download(url, output=archive, quiet=False)\n",
    "\n",
    "# # extract if not already extracted\n",
    "# if not os.path.isdir(extract_dir):\n",
    "#     with tarfile.open(archive, \"r:gz\") as tar:\n",
    "#         tar.extractall(path=dataset_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Preparing dataset with TerraTorch datamodule\n",
    "Check TerraTorch details in [publication](https://arxiv.org/pdf/2503.20563) and in the [repository](https://github.com/IBM/terratorch)\n",
    "\n",
    "- Point `dataset_path` to the location of the dataset\n",
    "- The dataset has already been pre-processed in the format that the TerraTorch datamodule can accept\n",
    "- We are using the `GenericMultiModalDataModule` in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Setting up the datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/Users/samuel.omole/Desktop/repos/geofm_datasets/sen1floods11_v1.1\") # path to dataset\n",
    "\n",
    "datamodule = terratorch.datamodules.GenericMultiModalDataModule(\n",
    "    task=\"segmentation\",\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    num_classes=2,\n",
    "    # Define input modalities. The names must match the keys in the dicts below and everywhere.\n",
    "    modalities=[\"S2L1C\", \"S1GRD\"],\n",
    "    rgb_modality=\"S2L1C\",  # Used for plotting. Defaults to the first modality if not provided.\n",
    "    rgb_indices=[3,2,1],  # RGB channel positions in the rgb_modality.\n",
    "\n",
    "    # Define data paths as dicts using the modality names as keys.\n",
    "    train_data_root={\n",
    "        \"S2L1C\": dataset_path / \"data/S2L1CHand\",\n",
    "        \"S1GRD\": dataset_path / \"data/S1GRDHand\",\n",
    "    },\n",
    "    train_label_data_root=dataset_path / \"data/LabelHand\",\n",
    "    val_data_root={\n",
    "        \"S2L1C\": dataset_path / \"data/S2L1CHand\",\n",
    "        \"S1GRD\": dataset_path / \"data/S1GRDHand\",\n",
    "    },\n",
    "    val_label_data_root=dataset_path / \"data/LabelHand\",\n",
    "    test_data_root={\n",
    "        \"S2L1C\": dataset_path / \"data/S2L1CHand\",\n",
    "        \"S1GRD\": dataset_path / \"data/S1GRDHand\",\n",
    "    },\n",
    "    test_label_data_root=dataset_path / \"data/LabelHand\",\n",
    "\n",
    "    # Define split files\n",
    "    train_split=dataset_path / \"splits/flood_train_data.txt\",\n",
    "    val_split=dataset_path / \"splits/flood_valid_data.txt\",\n",
    "    test_split=dataset_path / \"splits/flood_test_data.txt\",\n",
    "    \n",
    "    # Define suffix\n",
    "    image_grep={\n",
    "        \"S2L1C\": \"*_S2Hand.tif\",\n",
    "        \"S1GRD\": \"*_S1Hand.tif\",\n",
    "    },\n",
    "    label_grep=\"*_LabelHand.tif\",\n",
    "    \n",
    "    # You can select a subset of the dataset bands as model inputs by providing dataset_bands and output_bands.\n",
    "    # This setting is optional for all modalities and needs to be provided as dicts.\n",
    "    # Here is an example for with S-1 GRD. You could change the output to [\"VV\"] to only train on the first band.\n",
    "    dataset_bands={\n",
    "        \"S1GRD\": [\"VV\", \"VH\"]\n",
    "    },\n",
    "    output_bands={\n",
    "        \"S1GRD\": [\"VV\", \"VH\"]\n",
    "    },\n",
    "\n",
    "    # Define standardization values. We use the pre-training values provided for the TerraMind model\n",
    "    # Note that means and stds must be aligned with the output_bands defined earlier (equal length of values).\n",
    "    # For the S-2 L1C where all the standardization values are provided, the dataset and output bands were not specified earlier\n",
    "    means={\n",
    "      \"S2L1C\": [2357.089, 2137.385, 2018.788, 2082.986, 2295.651, 2854.537, 3122.849, 3040.560, 3306.481, 1473.847, 506.070, 2472.825, 1838.929],\n",
    "    #   \"S2L2A\": [1390.458, 1503.317, 1718.197, 1853.910, 2199.100, 2779.975, 2987.011, 3083.234, 3132.220, 3162.988, 2424.884, 1857.648],\n",
    "      \"S1GRD\": [-12.599, -20.293],\n",
    "    #   \"S1RTC\": [-10.93, -17.329],\n",
    "    #   \"RGB\": [87.271, 80.931, 66.667],\n",
    "    #   \"DEM\": [670.665]\n",
    "    },\n",
    "    stds={\n",
    "      \"S2L1C\": [1624.683, 1675.806, 1557.708, 1833.702, 1823.738, 1733.977, 1732.131, 1679.732, 1727.26, 1024.687, 442.165, 1331.411, 1160.419],\n",
    "    #   \"S2L2A\": [2106.761, 2141.107, 2038.973, 2134.138, 2085.321, 1889.926, 1820.257, 1871.918, 1753.829, 1797.379, 1434.261, 1334.311],\n",
    "      \"S1GRD\": [5.195, 5.890],\n",
    "    #   \"S1RTC\": [4.391, 4.459],\n",
    "    #   \"RGB\": [58.767, 47.663, 42.631],\n",
    "    #   \"DEM\": [951.272],\n",
    "    },\n",
    "    \n",
    "    # Apply albumentations to augment the dataset\n",
    "    train_transform=[\n",
    "        albumentations.D4(), # Performs random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "    ],\n",
    "    val_transform=None,  # Applies ToTensorV2() by default if not provided\n",
    "    test_transform=None,\n",
    "    \n",
    "    no_label_replace=-1,  # Replace NaN labels. defaults to -1 which is ignored in the loss and metrics.\n",
    "    no_data_replace=0,  # Replace NaN data\n",
    ")\n",
    "\n",
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Plotting some training and validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datamodule.val_dataset\n",
    "train_dataset = datamodule.train_dataset\n",
    "train_dataset.plot(train_dataset[50]) # to show some random plots of the training data\n",
    "plt.show()\n",
    "train_dataset.plot(train_dataset[57])\n",
    "plt.show()\n",
    "train_dataset.plot(train_dataset[200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.plot(val_dataset[8])\n",
    "plt.show()\n",
    "val_dataset.plot(val_dataset[15])\n",
    "plt.show()\n",
    "val_dataset.plot(val_dataset[68])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking test dataset size\n",
    "# required later on when plotting test set predictions\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Exploring the [TerraTorch](https://ibm.github.io/terratorch/quick_start/) model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just prints out all available TerraMind backbones in the registry.  \n",
    "[backbone\n",
    " for backbone in TERRATORCH_BACKBONE_REGISTRY\n",
    " if 'terramind' in backbone\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available decoders\n",
    "list(TERRATORCH_DECODER_REGISTRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a glance at the model and its architecture\n",
    "model = BACKBONE_REGISTRY.build(\"terramind_v1_small\", pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Building TerraMind and fine-tuning via PyTorch Lightning\n",
    "Refer to the [publication](https://arxiv.org/pdf/2504.11171) and [repository](https://github.com/IBM/terramind/tree/main) for more details\n",
    "- This section sets up the trainer for fine-tuning the model on the dataset\n",
    "- The paths to store the logging details and checkpoint need to be provided\n",
    "- The decoder layer parameters are updated while the model backbone are frozen\n",
    "- Training for a set number of epochs (50 is shown below but change to lower number to quickly test the trainer set up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Setting up the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "# By default, TerraTorch saves the model with the best validation loss.\n",
    "# You can overwrite this by defining a custom ModelCheckpoint, e.g., saving the model with the highest validation mIoU.  \n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../output/terramind_small_sen1floods11/checkpoints/\", # Change as appropriate\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/mIoU\", # Variable to monitor\n",
    "    filename=\"best-mIoU\",\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "# Set up the lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\", # set to gpu if you have one\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
    "    precision=32, # Note: setting precision as \"16-mixed\" speeds up training with half precision\n",
    "    num_nodes=1,\n",
    "    logger=True,  # Uses TensorBoard by default\n",
    "    max_epochs=50, # The higher the number of epoch the longer the training process and vice-versa\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"../output/terramind_small_sen1floods11/\", # Change as appropriate\n",
    ")\n",
    "\n",
    "# Segmentation task that builds the model and handles training and validation steps.  \n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",  # Combines a backbone with necks, the decoder, and a head\n",
    "    model_args={\n",
    "        # TerraMind backbone\n",
    "        \"backbone\": \"terramind_v1_small\", # change to specific model e.g., for large version: terramind_v1_large \n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_modalities\": [\"S2L1C\", \"S1GRD\"],\n",
    "        # Optionally, define the input bands. This is only needed if you select a subset of the pre-training bands, as explained above.\n",
    "        # \"backbone_bands\": {\"S1GRD\": [\"VV\"]},\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                \"indices\": [2, 5, 8, 11] # indices for terramind_v1_base & small\n",
    "                # \"indices\": [5, 11, 17, 23] # indices for terramind_v1_large\n",
    "            },\n",
    "            {\"name\": \"ReshapeTokensToImage\",\n",
    "             \"remove_cls_token\": False},  # TerraMind is trained without CLS token, which needs to be specified.\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"}  # Some decoders like UNet or UperNet expect hierarchical features.\n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 2, # there are two classes in the mask label image\n",
    "    },\n",
    "    \n",
    "    loss=\"dice\",  # dice is recommended for binary tasks and ce for multi-class tasks. \n",
    "    optimizer=\"AdamW\",\n",
    "    lr=2e-5,  # We can perform hyperparameter optimization using terratorch-iterate but we have demonstrated that  \n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True, # Setting as True speeds up fine-tuning. It is recommended to fine-tune the backbone as well for the best performance. \n",
    "    freeze_decoder=False, # Should be false to update the decoder layer parameters\n",
    "    plot_on_val=True,  # Plot predictions during validation steps  \n",
    "    class_names=[\"Others\", \"Water\"]  # optionally define class names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Evaluate the model performance on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "# This prints out the test metrics to evaluate the performance of the model on the test dataset\n",
    "best_ckpt_path = \"../output/terramind_small_sen1floods11/checkpoints/best-mIoU.ckpt\" # Change as appropriate\n",
    "trainer.test(model,\n",
    "             datamodule=datamodule,\n",
    "             ckpt_path=best_ckpt_path,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Predicting some example test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This predicts and plots some example test set batch\n",
    "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "    best_ckpt_path,\n",
    "    model_factory=model.hparams.model_factory,\n",
    "    model_args=model.hparams.model_args,\n",
    ")\n",
    "\n",
    "test_loader = datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader)) # this only selects the first batch in the test_dataloader\n",
    "    images = batch[\"image\"]\n",
    "    for mod, value in images.items():\n",
    "        images[mod] = value.to(model.device)\n",
    "    masks = batch[\"mask\"].numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(8): # 8 is the batch size so set this to <= 8\n",
    "    sample = {\n",
    "        \"image\": batch[\"image\"][\"S2L1C\"][i].cpu(),\n",
    "        \"mask\": batch[\"mask\"][i],\n",
    "        \"prediction\": preds[i],\n",
    "    }\n",
    "    test_dataset.plot(sample)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geofm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
