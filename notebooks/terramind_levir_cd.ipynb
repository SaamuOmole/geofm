{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import terratorch\n",
    "import albumentations as A\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from terratorch.registry import BACKBONE_REGISTRY, TERRATORCH_BACKBONE_REGISTRY, TERRATORCH_DECODER_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Download dataset and format it\n",
    "\n",
    "- The levir-cd+ dataset can be downloaded from [TorchGeo datasets](https://torchgeo.readthedocs.io/en/latest/api/datasets.html#id11)\n",
    "- Check `levir_cd.ipynb` notebook for how to download dataset and view samples of it\n",
    "- First run `convert_levircdplus_to_genericnongeosegdatamodule.py` to format dataset for the TerraTorch datamodule (this notebook will pointing at this structured directory)\n",
    "- Also run `plot_levircdplus.py` to visualise random stacked image and equivalent mask\n",
    "- Uncomment cell below to compute the means & stds for standardization of the 3 RGB channels (this imports `compute_stats_for_stacked_tifs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the means and stds per channel (pointing at the restructured directory)\n",
    "# from geofm.compute_levircdplus_means_stds import compute_stats_for_stacked_tifs\n",
    "\n",
    "# train_images_dir = Path(\"/Users/samuel.omole/Desktop/repos/geofm_datasets/levircdplus_restructured\")\n",
    "# means, stds  = compute_stats_for_stacked_tifs(train_images_dir)\n",
    "# print(\"means:\", means)\n",
    "# print(\"stds:\", stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â The means and stds, so they don't have to be recomputed every time\n",
    "means = [100.09773106949002, 98.8373331565483, 84.30711440011567]\n",
    "stds = [48.06710882295874, 45.49288485657313, 42.16697994338281]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Preparing the datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_binary(mask, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper code to convert masked images to binary\n",
    "    Map any positive pixel to 1, zero stays 0\n",
    "    \"\"\"\n",
    "    return (mask > 0).astype(\"uint8\")\n",
    "\n",
    "def check_label_values(datamodule, max_batches=20):\n",
    "    \"\"\"\n",
    "    Helper code to confirm mask_to_binary has converted mask labels to binary\n",
    "\n",
    "    Args:\n",
    "        datamodule: The TerraTorch datamodule\n",
    "        max_batches (int, optional): A random max number of batches to go through. Defaults to 20.\n",
    "    \"\"\"\n",
    "    datamodule.setup(\"fit\")\n",
    "    loader = datamodule.train_dataloader()\n",
    "    vals = set()\n",
    "    for i, batch in enumerate(loader):\n",
    "        # Try common batch formats\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "            _, y = batch[0], batch[1]\n",
    "        elif isinstance(batch, dict):\n",
    "            # Try common mask keys if dataset has different naming convention\n",
    "            for k in (\"mask\", \"masks\", \"label\", \"labels\", \"mask_target\"):\n",
    "                if k in batch:\n",
    "                    y = batch[k]\n",
    "                    break\n",
    "            else:\n",
    "                # Fallback to pick the second tensor-like item\n",
    "                tensor_items = [v for v in batch.values() if isinstance(v, torch.Tensor)]\n",
    "                if len(tensor_items) >= 2:\n",
    "                    y = tensor_items[1]\n",
    "                elif len(tensor_items) == 1:\n",
    "                    y = tensor_items[0]\n",
    "                else:\n",
    "                    raise RuntimeError(\"Couldn't locate mask tensor in batch. Keys: \" + \", \".join(batch.keys()))\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected batch type: {type(batch)}\")\n",
    "\n",
    "        # If torch.Tensor or numpy array, convert to int\n",
    "        if isinstance(y, torch.Tensor):\n",
    "            uniq = torch.unique(y).cpu().numpy()\n",
    "        else:\n",
    "            uniq = np.unique(y)\n",
    "        vals.update([int(v) for v in uniq])\n",
    "        if i >= max_batches - 1:\n",
    "            break\n",
    "\n",
    "    print(\"Unique label values in first\", max_batches, \"batches:\", sorted(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point dataset path to restructured directory\n",
    "dataset_path = Path(\"/Users/samuel.omole/Desktop/repos/geofm_datasets/levircdplus_restructured\")\n",
    "\n",
    "train_transform = [\n",
    "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),\n",
    "        A.D4(), # Random flips and rotation\n",
    "        A.Lambda(mask=mask_to_binary), \n",
    "        A.pytorch.transforms.ToTensorV2(),\n",
    "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=2),\n",
    "    ]\n",
    "\n",
    "val_transform = [\n",
    "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),\n",
    "        # A.D4(), # Random flips and rotation\n",
    "        A.Lambda(mask=mask_to_binary),\n",
    "        A.pytorch.transforms.ToTensorV2(),\n",
    "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=2),\n",
    "    ]\n",
    "\n",
    "datamodule = terratorch.datamodules.GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    num_classes=2,\n",
    "    # Define dataset paths, having restructured the original dataset\n",
    "    train_data_root=dataset_path / 'train' / 'images',\n",
    "    train_label_data_root=dataset_path / 'train' / 'labels',\n",
    "    val_data_root=dataset_path / 'val' / 'images',\n",
    "    val_label_data_root=dataset_path / 'val' / 'labels',\n",
    "    test_data_root=dataset_path / 'test' / 'images',\n",
    "    test_label_data_root=dataset_path / 'test' / 'labels',\n",
    "    \n",
    "    img_grep='*_stacked.tif',\n",
    "    label_grep='*_mask.png',\n",
    "    \n",
    "    dataset_bands=[\"BLUE\",\"GREEN\",\"RED\"], \n",
    "    output_bands=[\"BLUE\",\"GREEN\",\"RED\"],\n",
    "    \n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform, \n",
    "    test_transform=val_transform, # Apply the same transform as validation set\n",
    "    expand_temporal_dimension=True,\n",
    "    means=means,\n",
    "    stds=stds,\n",
    "    no_label_replace=-1,\n",
    "    no_data_replace=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the labels are binary\n",
    "check_label_values(datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Building the TerraMind model and fine-tuning with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Setting up the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0) # Set seed for reproducibility\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../output_levirpluscd/terramind_small/checkpoints/\", # Change as appropriate\n",
    "    mode=\"min\",\n",
    "    monitor=\"val/loss\", # Variable to monitor\n",
    "    filename=\"best-loss\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, \n",
    "    precision='32',\n",
    "    num_nodes=1,\n",
    "    logger=True,\n",
    "    max_epochs=5, # For demos\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"../output_levirpluscd/terramind_small/\", # Change as appropriate\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"terramind_v1_small\",\n",
    "        \"backbone_pretrained\": True,\n",
    "        # \"backbone_in_channels\": 6,\n",
    "        \"backbone_use_temporal\": True,\n",
    "        \"backbone_temporal_pooling\": 'diff',\n",
    "        \"backbone_temporal_concat\": False,\n",
    "        \"backbone_modalities\": [\"RGB\"], #[\"S2L2A\"],\n",
    "        \"backbone_bands\": {\"RGB\": [\"RED\",\"GREEN\",\"BLUE\"]}, \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                \"indices\": [2, 5, 8, 11]\n",
    "            },\n",
    "            {\"name\": \"ReshapeTokensToImage\",},\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"}            \n",
    "        ],\n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 2,\n",
    "    },\n",
    "    \n",
    "    loss=\"dice\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True, \n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    "    class_names=['change', 'no change']  # Optionally define class names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Evaluate the model performance on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"../output_levirpluscd/terramind_small/checkpoints/best-loss.ckpt\" # Change path to saved model\n",
    "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required later on when plotting test set predictions\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Predicting & plotting some example test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_CHANNEL_COUNTS = (1, 2, 3, 4, 6)\n",
    "\n",
    "def ensure_channel_first(img, n_timesteps=None, pick_time='last'):\n",
    "    \"\"\"\n",
    "    Return an image numpy array with shape (C, H, W) that the\n",
    "    dataloader can use for plotting\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(img):\n",
    "        arr = img.cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(img)\n",
    "    arr = np.squeeze(arr)\n",
    "\n",
    "    # 2D array (1, H, W)\n",
    "    if arr.ndim == 2:\n",
    "        return arr[np.newaxis, :, :].astype(np.float32)\n",
    "\n",
    "    # 3D array - could be (C,H,W) or (H,W,C) or (T,H,W)\n",
    "    if arr.ndim == 3:\n",
    "        a, b, c = arr.shape\n",
    "        # (C, H, W)\n",
    "        if a in COMMON_CHANNEL_COUNTS:\n",
    "            return arr.astype(np.float32)\n",
    "        # (H, W, C)\n",
    "        if c in COMMON_CHANNEL_COUNTS:\n",
    "            return arr.transpose(2, 0, 1).astype(np.float32)\n",
    "        # (T, H, W)\n",
    "        if a <= 50 and (n_timesteps is None or a == n_timesteps):\n",
    "            if pick_time == 'first':\n",
    "                chosen = arr[0]\n",
    "            elif pick_time == 'last':\n",
    "                chosen = arr[-1]\n",
    "            else:\n",
    "                chosen = arr.mean(axis=0)\n",
    "            return chosen[np.newaxis, :, :].astype(np.float32)\n",
    "        # fallback assume (H, W, C)\n",
    "        return arr.transpose(2, 0, 1).astype(np.float32)\n",
    "\n",
    "    # 4D array - Handle all possible permutations\n",
    "    if arr.ndim == 4:\n",
    "        d0, d1, d2, d3 = arr.shape\n",
    "        t_idx = -1 if pick_time == 'last' else 0\n",
    "\n",
    "        # (C, T, H, W)\n",
    "        if d0 in COMMON_CHANNEL_COUNTS and d1 <= 50:\n",
    "            out = arr[:, t_idx, :, :]\n",
    "            return out.astype(np.float32)\n",
    "\n",
    "        # (T, C, H, W)\n",
    "        if d1 in COMMON_CHANNEL_COUNTS and d0 <= 50:\n",
    "            out = arr[t_idx, :, :, :]\n",
    "            return out.astype(np.float32)\n",
    "\n",
    "        # (H, W, C, T)\n",
    "        if d2 in COMMON_CHANNEL_COUNTS and d3 <= 50:\n",
    "            out = arr[:, :, :, t_idx]           # (H, W, C)\n",
    "            return out.transpose(2, 0, 1).astype(np.float32)\n",
    "\n",
    "        # (H, W, T, C)\n",
    "        if d3 in COMMON_CHANNEL_COUNTS and d2 <= 50:\n",
    "            out = arr[:, :, t_idx, :]           # (H, W, C)\n",
    "            return out.transpose(2, 0, 1).astype(np.float32)\n",
    "\n",
    "        # Flattened (T*C, H, W) with known n_timesteps\n",
    "        if n_timesteps is not None and d0 % n_timesteps == 0:\n",
    "            c = d0 // n_timesteps\n",
    "            reshaped = arr.reshape(n_timesteps, c, d1, d2)  # (T, C, H, W)\n",
    "            out = reshaped[t_idx, :, :, :]\n",
    "            return out.astype(np.float32)\n",
    "\n",
    "        # Fallback to average over the first axis and hope for best\n",
    "        out = arr.mean(axis=0)\n",
    "        # Fine if this produced (C,H,W) else try transpose\n",
    "        if out.ndim == 3 and out.shape[0] in COMMON_CHANNEL_COUNTS:\n",
    "            return out.astype(np.float32)\n",
    "        if out.ndim == 3:\n",
    "            return out.transpose(2, 0, 1).astype(np.float32)\n",
    "\n",
    "    raise ValueError(f\"Unsupported image ndim={arr.ndim}, shape={arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(loader, seed=None):\n",
    "    \"\"\"\n",
    "    Gets random batch not just the first\n",
    "\n",
    "    Args:\n",
    "        loader (_type_): The test loader \n",
    "        seed (_type_, optional): The seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        The extracted batch from the test loader\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    n_batches = len(loader)\n",
    "    idx = random.randrange(n_batches)\n",
    "    batch = next(itertools.islice(iter(loader), idx, None))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "    best_ckpt_path,\n",
    "    model_factory=model.hparams.model_factory,\n",
    "    model_args=model.hparams.model_args,\n",
    ")\n",
    "\n",
    "test_loader = datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    # batch = next(iter(test_loader))\n",
    "    batch = get_random_batch(test_loader, seed=10)\n",
    "    images = batch[\"image\"].to(model.device)   # leave for inference\n",
    "    masks = batch[\"mask\"].cpu().numpy()\n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "# Plot example predictions\n",
    "for i in range(3):\n",
    "    raw_img = batch[\"image\"][i].cpu()   # torch tensor or numpy\n",
    "    print(raw_img.shape)\n",
    "    prepared_img = ensure_channel_first(raw_img, n_timesteps=2, pick_time='first')\n",
    "    print(prepared_img.shape)\n",
    "    print(f\"mask shape: {masks[i].shape}\")\n",
    "\n",
    "    sample = {\n",
    "        \"image\": prepared_img,\n",
    "        \"mask\": masks[i],\n",
    "        \"prediction\": preds[i], \n",
    "    }\n",
    "    test_dataset.plot(sample)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geofm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
